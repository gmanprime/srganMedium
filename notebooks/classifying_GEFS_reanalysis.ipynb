{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This .ipynb contains preprocessing codes for GEFS reforecast\n",
    "and GEFS reanalysis datasets\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Ej7j4WL8y_EX"
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "dh803EA3zynP"
   },
   "source": [
    "# Reanalysis and Reforecast 16 x 19 --> 8 x 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ffYrjzMVzJ8P"
   },
   "outputs": [],
   "source": [
    "# reading variable files\n",
    "PATH = '../Data/raw_data/' # change to your own path\n",
    "ds_reanalysis_tp = xr.open_dataset(PATH + 'GEFSv12-Reanalysis_tp_2000_2019.nc') # only 1 single reanalysis tp\n",
    "\n",
    "# slicing dimensions 8 x 11 grid points\n",
    "ds_reanalysis_tp = ds_reanalysis_tp.sel(lon=slice(102.5, 105.00), lat=slice(2.5,0.75))\n",
    "\n",
    "# starting from 2000-01-01 06:00:00 to 2019-12-31 18:00:00, total 29219 time steps\n",
    "ds_reanalysis_tp = ds_reanalysis_tp.isel(time=slice(1, None))\n",
    "del ds_reanalysis_tp.attrs['history'] # remove long history text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split into train val test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = ds_reanalysis_tp.sel(time=slice('2000-01-01T06', '2014-01-01T06'))\n",
    "y_val = ds_reanalysis_tp.sel(time=slice('2014-01-01T12', '2016-12-31T12'))\n",
    "y_test = ds_reanalysis_tp.sel(time=slice('2016-12-31T18', '2019-12-31T18'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: set()\n",
      "val: set()\n",
      "test: set()\n"
     ]
    }
   ],
   "source": [
    "def timecheck(train, val, test):\n",
    "    '''\n",
    "    Return missing time step in train, val and test split\n",
    "    '''\n",
    "    train_timecheck = np.arange(np.datetime64(\"2000-01-01T06\"), np.datetime64(\"2014-01-01T12\"), np.timedelta64(6, \"h\"))\n",
    "    val_timecheck = np.arange(np.datetime64(\"2014-01-01T12\"), np.datetime64(\"2016-12-31T18\"), np.timedelta64(6, \"h\"))\n",
    "    test_timecheck = np.arange(np.datetime64(\"2016-12-31T18\"), np.datetime64(\"2020-01-01T00\"), np.timedelta64(6, \"h\"))\n",
    "    print('train:', set(train_timecheck) - set(train.time.values.astype('datetime64[h]')))\n",
    "    print('val:', set(val_timecheck) - set(val.time.values.astype('datetime64[h]')))\n",
    "    print('test:', set(test_timecheck) - set(test.time.values.astype('datetime64[h]')))\n",
    "    return\n",
    "\n",
    "timecheck(y_train, y_val, y_test) # reanalysis are all ok, no missing dates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log transform and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log transform and min_max normalization for reanalysis_tp TRAINING DATASET\n",
    "scaler_train_tp = MinMaxScaler() \n",
    "y_one_col = y_train.tp.values.reshape([y_train.tp.values.shape[0]*y_train.tp.values.shape[1]*y_train.tp.values.shape[2], 1])\n",
    "y_one_col = np.log10(y_one_col+1) # 10**X_one_col - 1 to scale back\n",
    "y_one_col_res = scaler_train_tp.fit_transform(y_one_col) # scaler_train_apcp.inverse_transform(X_one_col_res) to scale back, or use 10**scaler_train_apcp.inverse_transform(X_one_col_res) -1 only\n",
    "y_train.tp.values = y_one_col_res.reshape(y_train.tp.values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_val_test(val_test, scaler_train, is_prec=True):\n",
    "    '''\n",
    "    Input (example): ds_val_apcp.tp, scaler_train_apcp, True/False\n",
    "    Output: Transformed validation/test XR data\n",
    "    If is_prec set to True, variable is precipitation\n",
    "    '''\n",
    "    if is_prec:\n",
    "        X_one_col = val_test.values.reshape([val_test.values.shape[0]*val_test.values.shape[1]*val_test.values.shape[2], 1])\n",
    "        X_one_col = np.log10(X_one_col+1)\n",
    "        X_one_col_res = scaler_train.transform(X_one_col)\n",
    "        val_test.values = X_one_col_res.reshape(val_test.values.shape)\n",
    "        return val_test.values\n",
    "\n",
    "    else:\n",
    "        X_one_col = val_test.values.reshape([val_test.values.shape[0]*val_test.values.shape[1]*val_test.values.shape[2], 1])\n",
    "        # X_one_col = np.log10(X_one_col+1)\n",
    "        X_one_col_res = scaler_train.transform(X_one_col)\n",
    "        val_test.values = X_one_col_res.reshape(val_test.values.shape)\n",
    "        return val_test.values\n",
    "\n",
    "# reanalysis\n",
    "y_val.tp.values = transform_val_test(y_val.tp, scaler_train_tp, True)\n",
    "y_test.tp.values = transform_val_test(y_test.tp, scaler_train_tp, True)\n",
    "\n",
    "def inverse_val_test(transformed_vt, scaler_train, is_prec=True):\n",
    "    '''\n",
    "    Input (example): ds_val_apcp.tp, scaler_train_apcp, True/False\n",
    "    Output: Inversed of transformed validation/test XR data\n",
    "    If is_prec set to True, variable is precipitation\n",
    "    '''\n",
    "    if is_prec:\n",
    "        X_one_col = transformed_vt.values.reshape([transformed_vt.values.shape[0]*transformed_vt.values.shape[1]*transformed_vt.values.shape[2], 1])\n",
    "        X_one_col_res = 10**scaler_train.inverse_transform(X_one_col) -1\n",
    "        transformed_vt.values = X_one_col_res.reshape(transformed_vt.values.shape)\n",
    "        return transformed_vt.values\n",
    "\n",
    "    else:\n",
    "        X_one_col = transformed_vt.values.reshape([transformed_vt.values.shape[0]*transformed_vt.values.shape[1]*transformed_vt.values.shape[2], 1])\n",
    "        X_one_col_res = scaler_train.inverse_transform(X_one_col)\n",
    "        transformed_vt.values = X_one_col_res.reshape(transformed_vt.values.shape)\n",
    "        return transformed_vt.values\n",
    "\n",
    "# retrieving back original precipitation\n",
    "# do not use this yet\n",
    "# ds_val_apcp.tp.values = inverse_val_test(ds_val_apcp.tp, scaler_train_apcp, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# converting reanalysis to classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_50 = np.quantile(y_train.tp.values, 0.5)\n",
    "quantile_75 = np.quantile(y_train.tp.values, 0.75)\n",
    "quantile_95 = np.quantile(y_train.tp.values, 0.95)\n",
    "\n",
    "\n",
    "# here we perform the binning of the precipitation data\n",
    "# 0: below 50th percentile\n",
    "# 1: between 50th and 75th percentile\n",
    "# 2: between 75th and 95th percentile\n",
    "# 3: above 95th percentile\n",
    "# we then use these values as the target for the classification task\n",
    "y_train.tp.values= np.array(pd.cut(y_train.tp.values.reshape(-1),\n",
    "                                bins=[\n",
    "                                    -0.1,\n",
    "                                    quantile_50,\n",
    "                                    quantile_75,\n",
    "                                    quantile_95,\n",
    "                                    1.1\n",
    "                                    ],\n",
    "                                labels=[0,1,2,3])).reshape(\n",
    "                                    y_train.tp.shape[0],\n",
    "                                    y_train.tp.shape[1],\n",
    "                                    y_train.tp.shape[2]\n",
    "                                    )\n",
    "\n",
    "y_val.tp.values= np.array(pd.cut(y_val.tp.values.reshape(-1),\n",
    "                                 bins=[\n",
    "                                     -0.1,\n",
    "                                     quantile_50,\n",
    "                                     quantile_75,\n",
    "                                     quantile_95,\n",
    "                                     1.1\n",
    "                                     ],\n",
    "                                 labels=[0,1,2,3])).reshape(\n",
    "                                     y_val.tp.shape[0],\n",
    "                                     y_val.tp.shape[1],\n",
    "                                     y_val.tp.shape[2]\n",
    "                                     )\n",
    "\n",
    "y_test.tp.values= np.array(pd.cut(y_test.tp.values.reshape(-1),\n",
    "                                bins=[\n",
    "                                    -0.1,\n",
    "                                    quantile_50,\n",
    "                                    quantile_75,\n",
    "                                    quantile_95,\n",
    "                                    1.1\n",
    "                                    ],\n",
    "                                labels=[0,1,2,3])).reshape(\n",
    "                                    y_test.tp.shape[0],\n",
    "                                    y_test.tp.shape[1],\n",
    "                                    y_test.tp.shape[2]\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20457, 8, 11, 1)\n",
      "(4381, 8, 11, 1)\n",
      "(4381, 8, 11, 1)\n"
     ]
    }
   ],
   "source": [
    "# training FOR REANALYSIS CLASSIFICATION\n",
    "# save the data for classification task\n",
    "y_class_train = y_train.tp.values\n",
    "y_class_train = y_class_train[..., np.newaxis]\n",
    "print(y_class_train.shape)\n",
    "np.save('../Data/Transitions/1. Classification Output/y_class_train.npy', y_class_train)\n",
    "\n",
    "# val\n",
    "# save the data for classification task\n",
    "y_class_val = y_val.tp.values\n",
    "y_class_val = y_class_val[..., np.newaxis]\n",
    "print(y_class_val.shape)\n",
    "np.save('../Data/Transitions/1. Classification Output/y_class_val.npy', y_class_val)\n",
    "\n",
    "# testing\n",
    "# save the data for classification task\n",
    "y_class_test = y_test.tp.values\n",
    "y_class_test = y_class_test[..., np.newaxis]\n",
    "print(y_class_test.shape)\n",
    "np.save('../Data/Transitions/1. Classification Output/y_class_test.npy', y_class_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
