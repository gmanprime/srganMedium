{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This .ipynb contains preprocessing codes for GEFS reforecast\n",
        "and GEFS reanalysis datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ej7j4WL8y_EX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\bobby\\anaconda3\\envs\\research\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
          ]
        }
      ],
      "source": [
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dh803EA3zynP"
      },
      "source": [
        "# Reanalysis and Reforecast 16 x 19 --> 8 x 11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ffYrjzMVzJ8P"
      },
      "outputs": [],
      "source": [
        "# reading variable files\n",
        "PATH = 'C:\\\\Users\\\\bobby\\\\Documents\\\\GitHub\\\\climateMedium\\\\raw_data\\\\' # change to your own path\n",
        "ds_reanalysis_tp = xr.open_dataset(PATH + 'GEFSv12-Reanalysis_tp_2000_2019.nc') # only 1 single reanalysis tp\n",
        "\n",
        "# slicing dimensions 8 x 11 grid points\n",
        "ds_reanalysis_tp = ds_reanalysis_tp.sel(lon=slice('102.5', '105.00'), lat=('2.5','2.25','2','1.75','1.5','1.25','1','0.75'))\n",
        "\n",
        "# starting from 2000-01-01 06:00:00 to 2019-12-31 18:00:00, total 29219 time steps\n",
        "ds_reanalysis_tp = ds_reanalysis_tp.isel(time=slice(1, None)) \n",
        "del ds_reanalysis_tp.attrs['history'] # remove long history text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# split into train val test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train = ds_reanalysis_tp.sel(time=slice('2000-01-01T06', '2014-01-01T06'))\n",
        "y_val = ds_reanalysis_tp.sel(time=slice('2014-01-01T12', '2016-12-31T12'))\n",
        "y_test = ds_reanalysis_tp.sel(time=slice('2016-12-31T18', '2019-12-31T18'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train: set()\n",
            "val: set()\n",
            "test: set()\n"
          ]
        }
      ],
      "source": [
        "def timecheck(train, val, test):\n",
        "    '''\n",
        "    Return missing time step in train, val and test split\n",
        "    '''\n",
        "    train_timecheck = np.arange(np.datetime64(\"2000-01-01T06\"), np.datetime64(\"2014-01-01T12\"), np.timedelta64(6, \"h\"))\n",
        "    val_timecheck = np.arange(np.datetime64(\"2014-01-01T12\"), np.datetime64(\"2016-12-31T18\"), np.timedelta64(6, \"h\"))\n",
        "    test_timecheck = np.arange(np.datetime64(\"2016-12-31T18\"), np.datetime64(\"2020-01-01T00\"), np.timedelta64(6, \"h\"))\n",
        "    print('train:', set(train_timecheck) - set(train.time.values.astype('datetime64[h]')))\n",
        "    print('val:', set(val_timecheck) - set(val.time.values.astype('datetime64[h]')))\n",
        "    print('test:', set(test_timecheck) - set(test.time.values.astype('datetime64[h]')))\n",
        "    return\n",
        "\n",
        "timecheck(y_train, y_val, y_test) # reanalysis are all ok, no missing dates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Log transform and normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# log transform and min_max normalization for reanalysis_tp TRAINING DATASET\n",
        "scaler_train_tp = MinMaxScaler() \n",
        "y_one_col = y_train.tp.values.reshape([y_train.tp.values.shape[0]*y_train.tp.values.shape[1]*y_train.tp.values.shape[2], 1])\n",
        "y_one_col = np.log10(y_one_col+1) # 10**X_one_col - 1 to scale back\n",
        "y_one_col_res = scaler_train_tp.fit_transform(y_one_col) # scaler_train_apcp.inverse_transform(X_one_col_res) to scale back, or use 10**scaler_train_apcp.inverse_transform(X_one_col_res) -1 only\n",
        "y_train.tp.values = y_one_col_res.reshape(y_train.tp.values.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def transform_val_test(val_test, scaler_train, is_prec=True):\n",
        "    '''\n",
        "    Input (example): ds_val_apcp.tp, scaler_train_apcp, True/False\n",
        "    Output: Transformed validation/test XR data\n",
        "    If is_prec set to True, variable is precipitation\n",
        "    '''\n",
        "    if is_prec == True:\n",
        "        X_one_col = val_test.values.reshape([val_test.values.shape[0]*val_test.values.shape[1]*val_test.values.shape[2], 1])\n",
        "        X_one_col = np.log10(X_one_col+1) \n",
        "        X_one_col_res = scaler_train.transform(X_one_col) \n",
        "        val_test.values = X_one_col_res.reshape(val_test.values.shape)\n",
        "        return val_test.values\n",
        "        \n",
        "    else:\n",
        "        X_one_col = val_test.values.reshape([val_test.values.shape[0]*val_test.values.shape[1]*val_test.values.shape[2], 1])\n",
        "        # X_one_col = np.log10(X_one_col+1) \n",
        "        X_one_col_res = scaler_train.transform(X_one_col) \n",
        "        val_test.values = X_one_col_res.reshape(val_test.values.shape)\n",
        "        return val_test.values\n",
        "\n",
        "# reanalysis\n",
        "y_val.tp.values = transform_val_test(y_val.tp, scaler_train_tp, True)\n",
        "y_test.tp.values = transform_val_test(y_test.tp, scaler_train_tp, True)\n",
        "\n",
        "def inverse_val_test(transformed_vt, scaler_train, is_prec=True):\n",
        "    '''\n",
        "    Input (example): ds_val_apcp.tp, scaler_train_apcp, True/False\n",
        "    Output: Inversed of transformed validation/test XR data\n",
        "    If is_prec set to True, variable is precipitation\n",
        "    '''\n",
        "    if is_prec == True:\n",
        "        X_one_col = transformed_vt.values.reshape([transformed_vt.values.shape[0]*transformed_vt.values.shape[1]*transformed_vt.values.shape[2], 1])\n",
        "        X_one_col_res = 10**scaler_train.inverse_transform(X_one_col) -1\n",
        "        transformed_vt.values = X_one_col_res.reshape(transformed_vt.values.shape)\n",
        "        return transformed_vt.values\n",
        "    \n",
        "    else:\n",
        "        X_one_col = transformed_vt.values.reshape([transformed_vt.values.shape[0]*transformed_vt.values.shape[1]*transformed_vt.values.shape[2], 1])\n",
        "        X_one_col_res = scaler_train.inverse_transform(X_one_col)\n",
        "        transformed_vt.values = X_one_col_res.reshape(transformed_vt.values.shape)\n",
        "        return transformed_vt.values\n",
        "\n",
        "# retrieving back original precipitation \n",
        "# do not use this yet\n",
        "# ds_val_apcp.tp.values = inverse_val_test(ds_val_apcp.tp, scaler_train_apcp, True) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# converting reanalysis to classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "quantile_50 = np.quantile(y_train.tp.values, 0.5)\n",
        "quantile_75 = np.quantile(y_train.tp.values, 0.75)\n",
        "quantile_95 = np.quantile(y_train.tp.values, 0.95)\n",
        "\n",
        "y_train.tp.values= np.array(pd.cut(y_train.tp.values.reshape(-1), \n",
        "                                bins=[-0.1, \n",
        "                                quantile_50,\n",
        "                                quantile_75,\n",
        "                                quantile_95,\n",
        "                                1.1], \n",
        "                                labels=[0,1,2,3])).reshape(\n",
        "                                y_train.tp.shape[0], \n",
        "                                y_train.tp.shape[1], \n",
        "                                y_train.tp.shape[2])\n",
        "\n",
        "y_val.tp.values= np.array(pd.cut(y_val.tp.values.reshape(-1), \n",
        "                                bins=[-0.1, \n",
        "                                quantile_50,\n",
        "                                quantile_75,\n",
        "                                quantile_95,\n",
        "                                1.1], \n",
        "                                labels=[0,1,2,3])).reshape(\n",
        "                                y_val.tp.shape[0], \n",
        "                                y_val.tp.shape[1], \n",
        "                                y_val.tp.shape[2])\n",
        "\n",
        "y_test.tp.values= np.array(pd.cut(y_test.tp.values.reshape(-1), \n",
        "                                bins=[-0.1, \n",
        "                                quantile_50,\n",
        "                                quantile_75,\n",
        "                                quantile_95,\n",
        "                                1.1], \n",
        "                                labels=[0,1,2,3])).reshape(\n",
        "                                y_test.tp.shape[0], \n",
        "                                y_test.tp.shape[1], \n",
        "                                y_test.tp.shape[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(20457, 8, 11, 1)\n",
            "(4381, 8, 11, 1)\n",
            "(4381, 8, 11, 1)\n"
          ]
        }
      ],
      "source": [
        "# training FOR REANALYSIS CLASSIFICATION\n",
        "y_class_train = y_train.tp.values\n",
        "y_class_train = y_class_train[..., np.newaxis]\n",
        "print(y_class_train.shape)\n",
        "np.save('C:\\\\Users\\\\bobby\\\\Documents\\\\GitHub\\\\climateMedium\\\\postprocessed_data\\\\y_class_train.npy', y_class_train)\n",
        "\n",
        "# val\n",
        "y_class_val = y_val.tp.values\n",
        "y_class_val = y_class_val[..., np.newaxis]\n",
        "print(y_class_val.shape)\n",
        "np.save('C:\\\\Users\\\\bobby\\\\Documents\\\\GitHub\\\\climateMedium\\\\postprocessed_data\\\\y_class_val.npy', y_class_val)\n",
        "\n",
        "# testing\n",
        "y_class_test = y_test.tp.values\n",
        "y_class_test = y_class_test[..., np.newaxis]\n",
        "print(y_class_test.shape)\n",
        "np.save('C:\\\\Users\\\\bobby\\\\Documents\\\\GitHub\\\\climateMedium\\\\postprocessed_data\\\\y_class_test.npy', y_class_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.1 ('research')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "287ad1a523a1a0c79ba273026018d677ef26436c7e0e825f5e9b183804324b70"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
