{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1734185351.098052   59912 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1734185351.326980   63972 gpu_backend_lib.cc:579] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.\n",
      "Searched for CUDA in the following directories:\n",
      "  ./cuda_sdk_lib\n",
      "  ipykernel_launcher.runfiles/cuda_nvcc\n",
      "  ipykern/cuda_nvcc\n",
      "  \n",
      "  /usr/local/cuda\n",
      "  /home/gmankali/.local/lib/python3.12/site-packages/tensorflow/python/platform/../../../nvidia/cuda_nvcc\n",
      "  /home/gmankali/.local/lib/python3.12/site-packages/tensorflow/python/platform/../../../../nvidia/cuda_nvcc\n",
      "  /home/gmankali/.local/lib/python3.12/site-packages/tensorflow/python/platform/../../cuda\n",
      "  .\n",
      "You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.\n",
      "W0000 00:00:1734185351.362153   63963 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1734185351.364106   63971 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1734185351.365801   63970 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1734185351.367611   63962 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1734185351.369638   63965 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1734185351.371088   63964 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1734185351.372354   63972 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1734185351.373675   63966 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1734185351.375530   63960 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1734185351.376846   63967 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1734185351.378329   63961 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1734185351.380373   63969 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1734185351.382014   63968 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 1 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/319 [00:00<?, ?it/s]I0000 00:00:1734185357.185839   59912 service.cc:148] XLA service 0x557fa0f0e3c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1734185357.185891   59912 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti with Max-Q Design, Compute Capability 7.5\n",
      "2024-12-14 17:09:17.288576: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "E0000 00:00:1734185357.844231   59912 cuda_dnn.cc:522] Loaded runtime CuDNN library: 9.1.0 but source was compiled with: 9.3.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "E0000 00:00:1734185358.007287   59912 cuda_dnn.cc:522] Loaded runtime CuDNN library: 9.1.0 but source was compiled with: 9.3.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2024-12-14 17:09:18.018816: W tensorflow/core/framework/op_kernel.cc:1841] OP_REQUIRES failed at xla_ops.cc:577 : FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.\n"
     ]
    }
   ],
   "source": [
    "from Network import Generator\n",
    "from tensorflow import keras\n",
    "from keras.layers import Activation, BatchNormalization, UpSampling2D, Flatten\n",
    "from keras.layers import Dense, Input, Conv2D, LeakyReLU, PReLU, add\n",
    "from keras.models import Model\n",
    "from keras.initializers import RandomNormal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Softmax\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from numpy import load\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import xarray as xr\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Set memory growth for GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "# Number of classes for classification\n",
    "n_classes = 4\n",
    "\n",
    "# Function to create a weighted categorical cross-entropy loss\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "   # reshape weights to a 1D tensor\n",
    "   weights = weights.reshape((1, 1, 1, n_classes))\n",
    "\n",
    "   # define an internal function for calculating the weighted categorical cross-entropy loss\n",
    "   # -> this is the function that will be called when the loss is evaluated\n",
    "   # -> it takes the true labels and the predicted labels as input and returns the loss\n",
    "   # -> labels refer to the catagories (0, 1, 2, 3) and not the one-hot encoded labels\n",
    "   def wcce(y_true, y_pred):\n",
    "      # use the keras backend to calculate the categorical cross-entropy loss\n",
    "      Kweights = K.constant(weights)\n",
    "      y_true = K.cast(y_true, y_pred.dtype)\n",
    "      return K.categorical_crossentropy(y_true, y_pred) * K.sum(y_true * Kweights, axis=-1)\n",
    "\n",
    "   return wcce\n",
    "\n",
    "# Class weights for the weighted categorical cross-entropy loss\n",
    "class_weights = np.array([4, 19, 23, 56])  # Inverse percentage of classes\n",
    "class_loss = weighted_categorical_crossentropy(weights=class_weights)\n",
    "\n",
    "# Function to create the Fractions Skill Score (FSS) loss\n",
    "def make_FSS_loss(mask_size):\n",
    "   def my_FSS_loss(y_true, y_pred):\n",
    "      # Discretize y_true and y_pred to binary values (0/1) or soft discretization\n",
    "      want_hard_discretization = False\n",
    "      cutoff = 0.5  # Cutoff value for discretization\n",
    "\n",
    "      if want_hard_discretization:\n",
    "         y_true_binary = tf.where(y_true > cutoff, 1.0, 0.0)\n",
    "         y_pred_binary = tf.where(y_pred > cutoff, 1.0, 0.0)\n",
    "      else:\n",
    "         c = 10  # Steepness of sigmoid function\n",
    "         y_true_binary = tf.math.sigmoid(c * (y_true - cutoff))\n",
    "         y_pred_binary = tf.math.sigmoid(c * (y_pred - cutoff))\n",
    "\n",
    "      # Calculate densities using average pooling\n",
    "      pool1 = tf.keras.layers.AveragePooling2D(pool_size=(mask_size, mask_size), strides=(1, 1), padding=\"same\")\n",
    "      y_true_density = pool1(y_true_binary)\n",
    "      n_density_pixels = tf.cast((tf.shape(y_true_density)[1] * tf.shape(y_true_density)[2]), tf.float32)\n",
    "\n",
    "      pool2 = tf.keras.layers.AveragePooling2D(pool_size=(mask_size, mask_size), strides=(1, 1), padding=\"same\")\n",
    "      y_pred_density = pool2(y_pred_binary)\n",
    "\n",
    "      # Calculate Mean Squared Error (MSE) for densities\n",
    "      MSE_n = tf.keras.losses.MeanSquaredError()(y_true_density, y_pred_density)\n",
    "\n",
    "      # Calculate reference MSE for normalization\n",
    "      O_n_squared_sum = tf.reduce_sum(tf.keras.layers.Flatten()(tf.keras.layers.Multiply()([y_true_density, y_true_density])))\n",
    "      M_n_squared_sum = tf.reduce_sum(tf.keras.layers.Flatten()(tf.keras.layers.Multiply()([y_pred_density, y_pred_density])))\n",
    "      MSE_n_ref = (O_n_squared_sum + M_n_squared_sum) / n_density_pixels\n",
    "\n",
    "      # Avoid division by zero\n",
    "      my_epsilon = tf.keras.backend.epsilon()\n",
    "      if want_hard_discretization:\n",
    "         if MSE_n_ref == 0:\n",
    "            return MSE_n\n",
    "         else:\n",
    "            return MSE_n / MSE_n_ref\n",
    "      else:\n",
    "         return MSE_n / (MSE_n_ref + my_epsilon)\n",
    "\n",
    "   return my_FSS_loss\n",
    "\n",
    "# Mask size for FSS loss\n",
    "mask_size = 3\n",
    "\n",
    "# Image shapes\n",
    "image_shape_hr = (96, 132, 1)  # High resolution image shape\n",
    "image_shape_lr = (8, 11, 13)  # Low resolution image shape\n",
    "downscale_factor = 12\n",
    "\n",
    "# Paths to data\n",
    "PATH = \"./Data/\"  # Change to your own path\n",
    "\n",
    "# Load training and validation data\n",
    "reforecast_train = load(PATH + \"X_train_ensemble.npy\")\n",
    "yhr_train = load(PATH + \"y_hr_train.npy\")\n",
    "reforecast_val = load(PATH + \"X_val_ensemble.npy\")\n",
    "yhr_val = load(PATH + \"y_hr_val.npy\")\n",
    "reanalysis_class_train = load(PATH + \"y_class_train.npy\")\n",
    "reanalysis_class_val = load(PATH + \"y_class_val.npy\")\n",
    "\n",
    "# Convert class labels to categorical\n",
    "reanalysis_class_train = to_categorical(reanalysis_class_train, num_classes=n_classes)\n",
    "reanalysis_class_val = to_categorical(reanalysis_class_val, num_classes=n_classes)\n",
    "\n",
    "# Training function\n",
    "def train(epochs, batch_size):\n",
    "   x_train_lr = reforecast_train\n",
    "   y_train_hr = yhr_train\n",
    "   y_train_class = reanalysis_class_train\n",
    "\n",
    "   x_val_lr = reforecast_val\n",
    "   y_val_hr = yhr_val\n",
    "   y_val_class = reanalysis_class_val\n",
    "\n",
    "   batch_count = int(x_train_lr.shape[0] / batch_size)\n",
    "\n",
    "   # Initialize and compile the generator model\n",
    "   generator = Generator(image_shape_lr).generator()\n",
    "   generator.compile(\n",
    "      loss=[class_loss, make_FSS_loss(mask_size)],\n",
    "      optimizer=Adam(learning_rate=0.0001, beta_1=0.9),\n",
    "      loss_weights=[0.01, 1.0],\n",
    "      metrics=[\"mae\", \"mse\"],\n",
    "   )\n",
    "\n",
    "   # Open a file to log losses\n",
    "   loss_file = open(\"losses.txt\", \"w+\")\n",
    "   loss_file.close()\n",
    "\n",
    "   # Training loop\n",
    "   for e in range(1, epochs + 1):\n",
    "      print(\"-\" * 15, \"Epoch %d\" % e, \"-\" * 15)\n",
    "\n",
    "      for _ in tqdm(range(batch_count)):\n",
    "         rand_nums = np.random.randint(0, x_train_lr.shape[0], size=batch_size)\n",
    "\n",
    "         x_lr = x_train_lr[rand_nums]\n",
    "         y_hr = y_train_hr[rand_nums]\n",
    "         y_class = y_train_class[rand_nums]\n",
    "\n",
    "         gen_loss = generator.train_on_batch(x_lr, [y_class, y_hr])\n",
    "\n",
    "      gen_loss = str(gen_loss)\n",
    "      val_loss = generator.evaluate(x_val_lr, [y_val_class, y_val_hr], verbose=0)\n",
    "      val_loss = str(val_loss)\n",
    "\n",
    "      # Log losses\n",
    "      loss_file = open(\"losses.txt\", \"a\")\n",
    "      loss_file.write(\"epoch%d : generator_loss = %s; validation_loss = %s\\n\" % (e, gen_loss, val_loss))\n",
    "      loss_file.close()\n",
    "\n",
    "      # Save model checkpoints\n",
    "      if e <= 10:\n",
    "         if e % 5 == 0:\n",
    "            generator.save(\"gen_model%d.h5\" % e)\n",
    "      else:\n",
    "         if e % 10 == 0:\n",
    "            generator.save(\"gen_model%d.h5\" % e)\n",
    "\n",
    "# Train the model\n",
    "train(5, 64)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
